name: Run Webscraper Scripts

on:
  # Run daily at 6:00 AM UTC (7:00 AM CET / 8:00 AM CEST)
  schedule:
    - cron: '0 6 * * *'

  # Allow manual triggering from GitHub UI
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: '**/requirements.txt'

      - name: Install dependencies
        run: |
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          else
            echo "requirements.txt fehlt â€“ installiere Minimal-Dependencies"
            pip install requests beautifulsoup4 lxml python-dateutil supabase streamlit icalendar python-dotenv urllib3
          fi

      - name: Run extract_locations_from_html.py
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: |
          python .scraper/extract_locations_from_html.py

      - name: Run scrape_sportangebote.py
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: |
          python .scraper/scrape_sportangebote.py

      - name: Run update_cancellations.py
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: |
          python .scraper/update_cancellations.py
